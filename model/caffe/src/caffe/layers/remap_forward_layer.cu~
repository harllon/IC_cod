#include <vector>
#include <iostream>
#include "caffe/layers/remap_forward_layer.hpp"
#include <curand.h>
#include <curand_kernel.h>

namespace caffe {

template<typename Dtype>
__device__ inline Dtype getvalue(Dtype* V, int x, int y, int c, int n, int C, int W, int H) {
	//if ( x < 0 || x > W - 1 || y < 0 || y > H - 1 || c < 0 || c > C - 1) 
	//	return 0;	
	x = max(0, min(W-1, x));
	y = max(0, min(H-1, y));
	c = max(0, min(C-1, c));
	return V[((n*C+c)*H+y)*W+x];
}

template<typename Dtype>
__device__ inline void addvalue(Dtype* V, int x, int y, int c, int n, int C, int W, int H, Dtype v) {
	if ( x < 0 || x > W - 1 || y < 0 || y > H - 1 || c < 0 || c > C - 1) 
		return;
	V[((n*C+c)*H+y)*W+x] += v;
}

template<typename Dtype>
__device__ inline void setvalue(Dtype* V, int x, int y, int c, int n, int C, int W, int H, Dtype v) {
	if ( x < 0 || x > W - 1 || y < 0 || y > H - 1 || c < 0 || c > C - 1) 
		return;
	V[((n*C+c)*H+y)*W+x] = v;
}


template <typename Dtype>
__global__ void RemapForward(const int nthreads,
	const Dtype* const Vb, const Dtype* const coords, 
	const Dtype* const delta, Dtype* const Vt,
	const int C, const int H0, const int H1,
	const int W0, const int W1) {
    
   // nthreads = N * C * H1 * W1
	CUDA_KERNEL_LOOP(index, nthreads) {
		const int n = index / W1 / H1 / C;
		const int c = (index / W1 / H1) % C;
		const int h = (index / W1) % H1;
		const int w = index % W1;
		Dtype x = w + coords[((n*2+0)*H1+h)*W1+w] * delta[n];
		Dtype y = h + coords[((n*2+1)*H1+h)*W1+w] * delta[n];
		int x0 = floor(x); int x1 = x0 + 1;
		int y0 = floor(y); int y1 = y0 + 1;		
		Dtype v = Vb[((n*C+c)*H0+h)*W0+w];
		setvalue(Vt, x0, y0, c, n, C, W1, H1, v);
		setvalue(Vt, x0, y1, c, n, C, W1, H1, v);
		setvalue(Vt, x1, y0, c, n, C, W1, H1, v);
		setvalue(Vt, x1, y1, c, n, C, W1, H1, v);
	} 
}


template <typename Dtype>
void RemapForwardLayer<Dtype>::Forward_gpu(const vector<Blob<Dtype>*>& bottom,
    								const vector<Blob<Dtype>*>& top) {
	const Dtype* Vb = bottom[0]->gpu_data();
	const Dtype* coords = bottom[1]->gpu_data();
	const Dtype* delta = bottom[2]->gpu_data();
	Dtype* Vt = top[0]->mutable_gpu_data();
	int N  = bottom[0]->num();
	int C  = bottom[0]->channels();
	int W0 = bottom[0]->width();
	int H0 = bottom[0]->height();
	int W1 = bottom[1]->width();
	int H1 = bottom[1]->height();	
   caffe_gpu_set(top[0]->count(), Dtype(0), Vt);

	int count = N * C * H1 * W1;
	RemapForward<Dtype><<<CAFFE_GET_BLOCKS(count), CAFFE_CUDA_NUM_THREADS>>>(
      count, Vb, coords, delta, Vt, C, H0, H1, W0, W1);
}



template <typename Dtype>
__global__ void RemapBackward(const int nthreads,
const Dtype* const Vb, const Dtype* const Vt, 
const Dtype* const coords, const Dtype* const delta,
const Dtype* const top_diff, Dtype* const b1_diff,
const int C, const int H0, const int H1,
const int W0, const int W1) {

    // nthreads = N * H1 * W1
	CUDA_KERNEL_LOOP(index, nthreads) {
		const int n = index / W1 / H1;
		const int h = (index / W1) % H1;
		const int w = index % W1;

		Dtype x = w + coords[((n*2+0)*H1+h)*W1+w] * delta[n];
		Dtype y = h + coords[((n*2+1)*H1+h)*W1+w] * delta[n];
		int x0 = floor(x); int y0 = floor(y);
		int x1 = x0 + 1;   int y1 = y0 + 1;
		Dtype wx = x - x0, wy = y - y0;

		for ( int c = 0; c < C; c++ ) {			
			Dtype dv00 = getvalue(top_diff, x0, y0, c, n, C, W1, H1);
			Dtype dv01 = getvalue(top_diff, x0, y1, c, n, C, W1, H1);
			Dtype dv10 = getvalue(top_diff, x1, y0, c, n, C, W1, H1);
			Dtype dv11 = getvalue(top_diff, x1, y1, c, n, C, W1, H1);
			Dtype dv = (1-wx)*(1-wy)*dv00 + (1-wx)*wy*dv01 + wx*(1-wy)*dv10 + wx*wy*dv11;
			// Gradients
			Dtype dx = getvalue(Vt, x1, y0, c, n, C, W1, H1) - getvalue(Vt, x0, y0, c, n, C, W1, H1);
			Dtype dy = getvalue(Vt, x0, y1, c, n, C, W1, H1) - getvalue(Vt, x0, y0, c, n, C, W1, H1);

			// Backprop for bottom[1] (coords)
			addvalue(b1_diff, w, h, 0, n, 2, W1, H1, -dx * delta[n] * dv);
			addvalue(b1_diff, w, h, 1, n, 2, W1, H1, -dy * delta[n] * dv);
		}
	}
}

template <typename Dtype>
void RemapForwardLayer<Dtype>::Backward_gpu(const vector<Blob<Dtype>*>& top,
    								const vector<bool>& propagate_down,
    								const vector<Blob<Dtype>*>& bottom) {

	const Dtype* Vb = bottom[0]->gpu_data();
	const Dtype* coords = bottom[1]->gpu_data();
	const Dtype* delta = bottom[2]->gpu_data();
	const Dtype* Vt = top[0]->gpu_data();
	const Dtype* top_diff = top[0]->gpu_diff();
	Dtype* b1_diff = bottom[1]->mutable_gpu_diff();
		
	int N = bottom[0]->num();
	int C = bottom[0]->channels();
	int W0 = bottom[0]->width();
	int H0 = bottom[0]->height();
	int W1 = bottom[1]->width();
	int H1 = bottom[1]->height();

	caffe_gpu_set(bottom[1]->count(), Dtype(0), b1_diff);
	/*if (propagate_down[1]) {		
		int count = N * W1 * H1;
		RemapBackward<Dtype><<<CAFFE_GET_BLOCKS(count), CAFFE_CUDA_NUM_THREADS>>>(
      	count, Vb, Vt, coords, delta, top_diff, b1_diff, C, H0, H1, W0, W1);
	}*/
}

INSTANTIATE_LAYER_GPU_FUNCS(RemapForwardLayer);

}  // namespace caffe
